{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c73439a4-699c-4b8e-b23c-c77c6ae55127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS learning_pradeep.new_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cffb7395-8cd1-4e90-b052-4a4ad5abc085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "data_customers = [\n",
    "    (1, \"Pradeep\",  \"India\",   \"Telangana\", \"2023-01-10\"),\n",
    "    (2, \"Rohit\",    \"India\",   \"Maharashtra\", \"2023-02-15\"),\n",
    "    (3, \"Sam\",      \"USA\",     \"Texas\", \"2023-03-01\"),\n",
    "    (4, \"John\",     \"USA\",     \"California\", \"2023-04-12\"),\n",
    "    (5, \"David\",    \"UK\",      \"London\", \"2023-05-20\"),\n",
    "    (6, \"GhostUser\",\"Canada\",  \"Ontario\", \"2023-06-01\")  # never orders\n",
    "]\n",
    "\n",
    "columns_customers = [\"customer_id\",\"customer_name\",\"country\",\"state\",\"created_date\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data_customers, columns_customers)\\\n",
    "    .withColumn(\"created_date\", F.to_date(\"created_date\"))\n",
    "\n",
    "df1.write.saveAsTable(\"learning_pradeep.new_data.customers_delta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91a66294-ce74-40b7-bb50-2eea2b8665db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_products = [\n",
    "    (101, \"iPhone\", \"Mobile\", 800),\n",
    "    (102, \"MacBook\", \"Laptop\", 1500),\n",
    "    (103, \"AirPods\", \"Audio\", 200),\n",
    "    (104, \"iPad\", \"Tablet\", 600),\n",
    "    (105, \"Apple Watch\", \"Wearable\", 400)\n",
    "]\n",
    "\n",
    "columns_products = [\"product_id\",\"product_name\",\"category\",\"price\"]\n",
    "\n",
    "df_products = spark.createDataFrame(data_products, columns_products)\n",
    "\n",
    "df_products.write.saveAsTable(\"learning_pradeep.new_data.products_delta\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de8a3c9a-ac87-451f-bb70-293ac314e040",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_orders = [\n",
    "    (1001, 1, 101, \"2024-01-01\", 800),\n",
    "    (1002, 1, 103, \"2024-01-05\", 200),\n",
    "    (1003, 2, 102, \"2024-01-10\", 1500),\n",
    "    (1004, 2, 101, \"2024-02-01\", 800),\n",
    "    (1005, 3, 104, \"2024-02-15\", 600),\n",
    "    (1006, 4, 105, \"2024-03-01\", 400),\n",
    "    (1007, 4, 103, \"2024-03-05\", 200),\n",
    "    (1008, 5, 101, \"2024-03-20\", 800),\n",
    "    \n",
    "    # Duplicate record to test dedupe\n",
    "    (1008, 5, 101, \"2024-03-20\", 800),\n",
    "\n",
    "    # Nulls for practice\n",
    "    (1009, None, 104, \"2024-04-01\", 600),\n",
    "    (1010, 3, None, \"2024-04-10\", 500)\n",
    "]\n",
    "\n",
    "columns_orders = [\"order_id\",\"customer_id\",\"product_id\",\"order_date\",\"amount\"]\n",
    "\n",
    "df2 = spark.createDataFrame(data_orders, columns_orders) \\\n",
    "    .withColumn(\"order_date\", F.to_date(\"order_date\"))\n",
    "\n",
    "df2.write.saveAsTable(\"learning_pradeep.new_data.orders_delta\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
